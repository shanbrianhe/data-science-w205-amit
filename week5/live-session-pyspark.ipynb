{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Spark\n",
    "\n",
    "\n",
    "### what is Spark?\n",
    "\n",
    "http://spark.apache.org/\n",
    "\n",
    "*Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.*\n",
    "\n",
    "This nicest thing about Spark, is the interface to Python using PySpark\n",
    "\n",
    "There is some language Scala, that Spark is built on.  Luckily you don't have to learn Scala.\n",
    "\n",
    "\n",
    "\n",
    "### how does it work?\n",
    "\n",
    "it uses a data structure called an RDD (Resilient Distributed Dataset) which is basically some fast, clever, magical way of saving data to disk instead of holding it all in memory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the Spark SQL shell instead of Hive\n",
    "\n",
    "1. some pre-reqs\n",
    "* start hive\n",
    "* start the hive metastore\n",
    "\n",
    "1. just use this instead and do the same stuff\n",
    "```\n",
    "/data/spark15/bin/spark-sql\n",
    "```\n",
    "\n",
    "1. run some commands but faster\n",
    "```\n",
    "show tables;\n",
    "select avg(age) from foo;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the Python interface\n",
    "\n",
    "1. launch it \n",
    "```\n",
    "/data/spark15/bin/pyspark\n",
    "```\n",
    "\n",
    "1. or launch ipython (much better)\n",
    "```\n",
    "IPYTHON=1 /data/spark15/bin/pyspark\n",
    "```\n",
    "\n",
    "1. examine the \"Spark context variable\" and the \"Hive context variable\"\n",
    "```\n",
    "sc\n",
    "sqlContext\n",
    "```\n",
    "\n",
    "1. get Hive context and SQL context manually \n",
    "```\n",
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)\n",
    "```\n",
    "\n",
    "1. see what tables we have \n",
    "```\n",
    "sqlContext.sql(\"show tables\").show()\n",
    "```\n",
    "\n",
    "1. run a simple sql command\n",
    "```\n",
    "rdd = sqlContext.sql(\"select * from user_info\")\n",
    "rdd.show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### access HDFS directly bypassing Hive\n",
    "\n",
    "1. read a text file directly from HDFS\n",
    "```\n",
    "f = sc.textFile(\"hdfs://localhost/user/w205/lab_3/user_data/userdata_lab.csv\")\n",
    "f.count()\n",
    "f.take(5)\n",
    "```\n",
    "\n",
    "1. read the same file from HDFS without giving full path\n",
    "```\n",
    "f = sc.textFile('lab_3/user_data/userdata_lab.csv')\n",
    "f.count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### access a local file direct bypassing Hive and HDFS\n",
    "\n",
    "1. notice that you can give a full path to a text file\n",
    "```            \n",
    "f = sc.textFile('file:///home/w205/data/stadiums.csv')\n",
    "f.count()\n",
    "f.take(5)\n",
    "```\n",
    "\n",
    "2. **there is no way to read a local file without full path**\n",
    "\n",
    "*Question: when and why would you want to read local vs HDFS files?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets write a little code\n",
    "\n",
    "1. read in a long list of words\n",
    "```\n",
    "words = sc.textFile(\"file:///usr/share/dict/words\")\n",
    "words.count()\n",
    "```\n",
    "\n",
    "1. use the **filter** function\n",
    "```\n",
    "some_words = words.filter(lambda w: w.startswith(\"spark\"))\n",
    "some_words.count()\n",
    "some_words.take(7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets try a join in spark\n",
    "\n",
    "1. read in some files\n",
    "```\n",
    "rdd1 = sc.textFile('lab_3/user_data/userdata_lab.csv').map(lambda x: x.split(\"\\t\"))\n",
    "rdd2 = sc.textFile('lab_3/weblog_data/weblog_lab.csv').map(lambda x: x.split(\"\\t\"))\n",
    "```\n",
    "\n",
    "1. convert to data frames\n",
    "\n",
    "1. try a join ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
